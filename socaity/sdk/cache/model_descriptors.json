[{"id": "d5a12199-179a-418a-8ccf-a628fe0c30a5", "created_at": "2025-06-09T06:50:17.264327+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["5b8616c5-3852-46f6-9124-2a7842f0a983"], "display_name": "topazlabs/video-upscale", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/8252807c-a5ac-4008-96a5-814201b4738a/topaz_img.png", "version": "f4dad23bbe2d0bf4736d2ea8c9156f1911d8eeb511c8d0bb390931e25caaef61", "model_desc": "Video Upscaling from Topaz Labs", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "9da1f83e-859e-4e60-9bf8-7dd7055f78da", "created_at": "2025-06-09T06:50:18.612875+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "kwaivgi/kling-v2-0", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/c39d026b-b155-4904-8404-fefa3a2ff305/replicate-prediction-xs2x7fjs.webp", "version": "03c47b845aed8a009e0f83a45be0a2100ca11a7077e667a33224a54e85b2965c", "model_desc": "Generate 5s and 10s videos in 720p resolution", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "f0cada06-4ce6-42ed-a56b-adbb24a1e96d", "created_at": "2025-06-09T06:50:19.476753+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/deepseek-ai/DeepSeek-V3", "origin_model_repository_url": null, "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "deepseek-ai/deepseek-v3", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/1fb6b760-d41b-4421-94f9-9539fc928fce/deepseek-v3-cover.jpg", "version": "5500ea20e92e133d8981dc6f5b820d2eedd9b52f4ab8a856fccd4ddbb992396e", "model_desc": "DeepSeek-V3-0324 is the leading non-reasoning model, a milestone for open source", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "955535a8-0dc0-4a78-8838-6c950e7153b8", "created_at": "2025-06-09T06:50:20.680244+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["cefb9085-8de0-46ed-88c6-e2708ef8b19e", "e4125ace-09cf-40c7-b168-0aba36d3c840"], "display_name": "minimax/voice-cloning", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/532f411a-6277-496a-8d95-937fe5c302ef/s5frcc1hn1rmc0cpmnva4x24mw.webp", "version": "aa25ee1296b5c036b003ef80d32c83983c522e8c7d6f108460bbb0af97ebe93a", "model_desc": "Clone voices to use with Minimax's speech-02-hd and speech-02-turbo", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "5a52e54e-0f4e-49d6-96a9-323e36c95f73", "created_at": "2025-06-09T06:50:22.075591+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["2a455e76-d55f-44fb-895c-9b49d7f18e92", "7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "pixverse/pixverse-v4", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/daa1c364-fa52-4a13-922a-1bea888d62da/replicate-prediction-t1wrshv5.webp", "version": "4fa1624d7e845a7e1f29bc67ed753d6c8acbb3049ea59759b8c62a4a1aaa7629", "model_desc": "Quickly generate smooth 5s or 8s videos at 540p, 720p or 1080p", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "0ce66561-b559-48d3-b390-5f13ee7bb300", "created_at": "2025-06-09T06:50:23.093854+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["cefb9085-8de0-46ed-88c6-e2708ef8b19e", "03d59d4b-1b52-4f10-a613-fe9cb7511a6d"], "display_name": "minimax/speech-02-hd", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/5c681b84-f36d-43aa-8b50-ae66ce49baa1/j8nyxm83p5rmc0cpjb2t4mwqg8.webp", "version": "29657f664032844b8f800486164cf26acb2507288e348133e78ae871a43211d0", "model_desc": "Text-to-Audio (T2A) that offers voice synthesis, emotional expression, and multilingual capabilities. Optimized for high-fidelity applications like voiceovers and audiobooks.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "aaefeedd-73b7-46e4-8a4e-59dc06b832c5", "created_at": "2025-06-09T06:50:24.07138+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["cefb9085-8de0-46ed-88c6-e2708ef8b19e", "03d59d4b-1b52-4f10-a613-fe9cb7511a6d"], "display_name": "minimax/speech-02-turbo", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/78449de2-5ac1-4608-961f-47b1a5b04105/veptn61cesrmc0cpjb286e4gar.webp", "version": "43b17801b02267d0baf70071ff440358f75499f20ad5c51118a2fdad14ba9b8c", "model_desc": "Text-to-Audio (T2A) that offers voice synthesis, emotional expression, and multilingual capabilities. Designed for real-time applications with low latency", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "d2492cc0-86bb-4989-be1a-0d2bf7726cb1", "created_at": "2024-12-09T18:39:26.192788+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/replicate/cog-flux", "origin_model_repository_url": "https://github.com/black-forest-labs/flux", "model_category": ["2a455e76-d55f-44fb-895c-9b49d7f18e92"], "display_name": "Flux-Schnell", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/flux_schnell.png", "version": null, "model_desc": "FLUX.1 [schnell] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. For more information, please read our blog post.\n\nKey Features\nCutting-edge output quality and competitive prompt following, matching the performance of closed source alternatives.\nTrained using latent adversarial diffusion distillation, FLUX.1 [schnell] can generate high-quality images in only 1 to 4 steps.\nReleased under the apache-2.0 licence, the model can be used for personal, scientific, and commercial purposes.", "used_models": ["flux", "flux-schnell"], "sort_idx_featured": 0, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "0a6fe8ab-741e-4b9a-b057-a410b29b4262", "created_at": "2024-10-06T11:03:40.984063+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/SocAIty/Retrieval-based-Voice-Conversion-FastAPI", "origin_model_repository_url": null, "model_category": ["e4125ace-09cf-40c7-b168-0aba36d3c840"], "display_name": "HQ_VoiceCloning", "display_img_url": null, "version": null, "model_desc": "Copy a voice realistic with just a 5 to 10minutes of clean audio sample of that person.", "used_models": ["HUBERT", "rmvpe"], "sort_idx_featured": null, "n_usages": 0, "is_public": false, "is_validated": false, "has_billing": true}, {"id": "629ca70b-aa17-4a8c-80b4-95d001fe0d72", "created_at": "2024-10-05T12:58:02.902229+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/SocAIty/face2face", "origin_model_repository_url": null, "model_category": ["1d8a18b4-cd0e-4768-95d1-0f0dc869fd10", "bf804793-5488-4426-83fa-33e5e3f0f6f6", "38e65477-6017-4157-be5c-64f6950e1f23", "1974b15c-9815-4d02-98c9-4a7ff84ce301", "0e50e16b-a3e3-443f-8bcd-d48b25d57ff6"], "display_name": "Face2Face", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/frontend_images/629ca70b-aa17-4a8c-80b4-95d001fe0d72.png", "version": "0.0.9", "model_desc": "Face2Face is a generative AI technology to swap faces (aka Deep Fake) in images from one to another. For example, you can swap your face with Mona Lisa or your favorite celebrity.\nWith this repository you can:\n\nSwap faces from one image to another.\nSwap faces in images and videos.\nFace embeddings: Create face embeddings. With these embeddings you can later swap faces just by using the name.\nWith face recognition: Swap faces with face recognition.\nFace restoration: Enhance image quality of a portrait with a face enhancer model.\nIdentify faces with face-recognition.", "used_models": null, "sort_idx_featured": 0, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "2f40382d-a340-4d54-b7fb-d959c2b9cb4c", "created_at": "2025-02-12T11:24:28.281465+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/chenxwh/insanely-fast-whisper", "origin_model_repository_url": "https://github.com/Vaibhavs10/insanely-fast-whisper", "model_category": ["a0f34b57-c452-4baf-912b-ce2f144550d8"], "display_name": "insanely-fast-whisper", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/insanely-fast-whisper_2f40382d-a340-4d54-b7fb-d959c2b9cb4c/display_img.png", "version": "00.00.00", "model_desc": "Insanely Fast Whisper is an optimized tool designed to transcribe audio files with remarkable speed and efficiency. Built upon OpenAI's Whisper large v3 model, it leverages technologies such as Hugging Face Transformers, Optimum, and Flash Attention 2 to achieve rapid transcription performance. \n\nKey Features:\n\nHigh-Speed Transcription: Capable of transcribing 150 minutes of audio in under 98 seconds on an NVIDIA A100 GPU, significantly accelerating the transcription process. \n\nCross-Platform Compatibility: Supports both CUDA-enabled devices and Apple's Metal Performance Shaders (MPS) for Mac users, ensuring broad accessibility. \n\nUser-Friendly CLI: Offers an intuitive command-line interface with customizable options to optimize transcription throughput. Users can access various settings by running insanely-fast-whisper --help. \n\nPotential Applications:\n\nRapid Transcription: Ideal for quickly converting large volumes of audio content into text, benefiting professionals in journalism, research, and content creation.\n\nReal-Time Processing: Suitable for applications requiring immediate transcription, such as live event captioning or rapid data analysis.\n\nResource Efficiency: Optimized for performance on both high-end GPUs and Mac systems, making it accessible for users with varying hardware capabilities.\n\nBy utilizing Insanely Fast Whisper, users can achieve high-speed, accurate transcriptions, enhancing productivity across various fields that rely on efficient audio-to-text conversion.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "862f4410-2a6c-4fd7-813a-c7b32fc93bbc", "created_at": "2025-02-11T22:47:25.505829+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/meta-llama/llama3", "origin_model_repository_url": "https://github.com/meta-llama/llama3", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "codellama-13b", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/llama3.png", "version": "00.00.00", "model_desc": "Meta's Llama 3 is the latest iteration in Meta AI's series of large language models (LLMs), designed to advance natural language understanding and generation. Building upon its predecessors, Llama 3 offers enhanced performance and efficiency, making it a powerful tool for various applications. \n\nKey Features:\n\nInstruction Following: Llama 3 excels at understanding and executing complex instructions, making it suitable for tasks that require step-by-step guidance. \n\nKnowledge and Reasoning: The model demonstrates strong capabilities in connecting diverse ideas and providing insightful answers, akin to conversing with a knowledgeable individual. \n\nPotential Applications:\n\nText Generation: Create high-quality content across various domains, including creative writing and professional communications.\n\nConversational AI: Develop chatbots capable of engaging in natural and informative dialogues.\n\nSummarization and Translation: Summarize lengthy documents and translate text between languages with high accuracy.\n\nCoding Assistance: Aid in code generation and debugging for software development projects.\n\nLlama 3 is available in multiple configurations, including models with 8 billion and 70 billion parameters, catering to a range of computational requirements. \n\nBy leveraging Llama 3, developers and researchers can build advanced AI applications that require sophisticated language understanding and generation capabilities.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "0bb7dc1b-67b9-41a4-a4e9-8e819c938ade", "created_at": "2025-01-25T07:12:46.190439+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/SocAIty/SpeechCraft", "origin_model_repository_url": "https://github.com/suno-ai/bark", "model_category": ["e4125ace-09cf-40c7-b168-0aba36d3c840", "cefb9085-8de0-46ed-88c6-e2708ef8b19e", "e4125ace-09cf-40c7-b168-0aba36d3c840"], "display_name": "SpeechCraft", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/speechcraft_icon.png", "version": "00.00.00", "model_desc": "Ever wanted to create natural sounding speech from text, clone a voice or sound like someone else? SpeechCraft is ideal for creating voiceovers, audiobooks, or just having fun.\n\nFeatures:\nText2speech synthesis with the \ud83d\udc36 Bark model of Suno.ai\nGenerate text in different languages\nSupports emotions & singing.\nSpeaker generation / embedding generation aka voice cloning\nVoice2voice synthesis: given an audio file, generate a new audio file with the voice of a different speaker.\nConvenient deployment ready web API with FastTaskAPI\nAutomatic download of models", "used_models": [], "sort_idx_featured": 0, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "2576debc-51dd-420c-a1bc-b026d108074f", "created_at": "2025-02-11T22:43:53.185731+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/meta-llama/llama3", "origin_model_repository_url": "https://github.com/meta-llama/llama3", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "meta-llama3-8b", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/llama3.png", "version": "00.00.00", "model_desc": "Meta's Llama 3 is the latest iteration in Meta AI's series of large language models (LLMs), designed to advance natural language understanding and generation. Building upon its predecessors, Llama 3 offers enhanced performance and efficiency, making it a powerful tool for various applications. \n\nKey Features:\n\nInstruction Following: Llama 3 excels at understanding and executing complex instructions, making it suitable for tasks that require step-by-step guidance. \n\nKnowledge and Reasoning: The model demonstrates strong capabilities in connecting diverse ideas and providing insightful answers, akin to conversing with a knowledgeable individual. \n\nPotential Applications:\n\nText Generation: Create high-quality content across various domains, including creative writing and professional communications.\n\nConversational AI: Develop chatbots capable of engaging in natural and informative dialogues.\n\nSummarization and Translation: Summarize lengthy documents and translate text between languages with high accuracy.\n\nCoding Assistance: Aid in code generation and debugging for software development projects.\n\nLlama 3 is available in multiple configurations, including models with 8 billion and 70 billion parameters, catering to a range of computational requirements. \n\nBy leveraging Llama 3, developers and researchers can build advanced AI applications that require sophisticated language understanding and generation capabilities.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "81697da1-b9b2-42fb-b3fc-5a845c6ce6d3", "created_at": "2025-02-06T15:16:08.236801+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/zsxkib/HunyuanVideo/tree/replicate", "origin_model_repository_url": "https://github.com/Tencent/HunyuanVideo", "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "Hunyuan-Video", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/Hunyuan-Video_135754f6-2c99-4ec2-8a3b-8f389a56803f/hunyuan_socaity_gpu.webp", "version": "00.00.00", "model_desc": "HunyuanVideo Text-to-Video Generation Model \ud83c\udfac\nHunyuanVideo is an advanced text-to-video generation model that can create high-quality videos from text descriptions. It features a comprehensive framework that integrates image-video joint model training and efficient infrastructure for large-scale model training and inference.\n\nThis deployment is parallelized across multiple GPUs using context parallel attention from the awesome ParaAttention repo.\n\nModel Description \u2728\nThis model is trained on a spatial-temporally compressed latent space and uses a large language model for text encoding. According to professional human evaluation results, HunyuanVideo outperforms previous state-of-the-art models in terms of text alignment, motion quality, and visual quality.\n\nKey features:\n\n\ud83c\udfa8 High-quality video generation from text descriptions\n\ud83d\udcd0 Support for various aspect ratios and resolutions\n\u270d\ufe0f Advanced prompt handling with a built-in rewrite system\n\ud83c\udfaf Stable motion generation and temporal consistency\nPredictions Examples \ud83d\udcab\nThe model works well for prompts like: - \u201cA cat walks on the grass, realistic style\u201d - \u201cA drone shot of mountains at sunset\u201d - \u201cA flower blooming in timelapse\u201d\n\nLimitations \u26a0\ufe0f\nGeneration time increases with video length and resolution\nHigher resolutions require more GPU memory\nSome complex motions may require prompt engineering for best results", "used_models": [], "sort_idx_featured": 0, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "0ccac104-4634-4c97-9fbd-5a3853126df9", "created_at": "2025-02-11T22:48:44.699032+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/meta-llama/llama3", "origin_model_repository_url": "https://github.com/meta-llama/llama3", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "meta-llama-3-70b-instruct", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/llama3.png", "version": "00.00.00", "model_desc": "Meta's Llama 3 is the latest iteration in Meta AI's series of large language models (LLMs), designed to advance natural language understanding and generation. Building upon its predecessors, Llama 3 offers enhanced performance and efficiency, making it a powerful tool for various applications. \n\nKey Features:\n\nInstruction Following: Llama 3 excels at understanding and executing complex instructions, making it suitable for tasks that require step-by-step guidance. \n\nKnowledge and Reasoning: The model demonstrates strong capabilities in connecting diverse ideas and providing insightful answers, akin to conversing with a knowledgeable individual. \n\nPotential Applications:\n\nText Generation: Create high-quality content across various domains, including creative writing and professional communications.\n\nConversational AI: Develop chatbots capable of engaging in natural and informative dialogues.\n\nSummarization and Translation: Summarize lengthy documents and translate text between languages with high accuracy.\n\nCoding Assistance: Aid in code generation and debugging for software development projects.\n\nLlama 3 is available in multiple configurations, including models with 8 billion and 70 billion parameters, catering to a range of computational requirements. \n\nBy leveraging Llama 3, developers and researchers can build advanced AI applications that require sophisticated language understanding and generation capabilities.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "ed7aa59b-4f1d-45eb-a321-a7d8b751b1b7", "created_at": "2025-02-11T22:47:49.316022+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/meta-llama/llama3", "origin_model_repository_url": "https://github.com/meta-llama/llama3", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "codellama-70b-python", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/llama3.png", "version": "00.00.00", "model_desc": "Meta's Llama 3 is the latest iteration in Meta AI's series of large language models (LLMs), designed to advance natural language understanding and generation. Building upon its predecessors, Llama 3 offers enhanced performance and efficiency, making it a powerful tool for various applications. \n\nKey Features:\n\nInstruction Following: Llama 3 excels at understanding and executing complex instructions, making it suitable for tasks that require step-by-step guidance. \n\nKnowledge and Reasoning: The model demonstrates strong capabilities in connecting diverse ideas and providing insightful answers, akin to conversing with a knowledgeable individual. \n\nPotential Applications:\n\nText Generation: Create high-quality content across various domains, including creative writing and professional communications.\n\nConversational AI: Develop chatbots capable of engaging in natural and informative dialogues.\n\nSummarization and Translation: Summarize lengthy documents and translate text between languages with high accuracy.\n\nCoding Assistance: Aid in code generation and debugging for software development projects.\n\nLlama 3 is available in multiple configurations, including models with 8 billion and 70 billion parameters, catering to a range of computational requirements. \n\nBy leveraging Llama 3, developers and researchers can build advanced AI applications that require sophisticated language understanding and generation capabilities.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "07254c61-710a-4591-bf12-55df7e8a5b07", "created_at": "2025-02-11T22:48:24.775221+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/meta-llama/llama3", "origin_model_repository_url": "https://github.com/meta-llama/llama3", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "meta-llama-3-8b-instruct", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/llama3.png", "version": "00.00.00", "model_desc": "Meta's Llama 3 is the latest iteration in Meta AI's series of large language models (LLMs), designed to advance natural language understanding and generation. Building upon its predecessors, Llama 3 offers enhanced performance and efficiency, making it a powerful tool for various applications. \n\nKey Features:\n\nInstruction Following: Llama 3 excels at understanding and executing complex instructions, making it suitable for tasks that require step-by-step guidance. \n\nKnowledge and Reasoning: The model demonstrates strong capabilities in connecting diverse ideas and providing insightful answers, akin to conversing with a knowledgeable individual. \n\nPotential Applications:\n\nText Generation: Create high-quality content across various domains, including creative writing and professional communications.\n\nConversational AI: Develop chatbots capable of engaging in natural and informative dialogues.\n\nSummarization and Translation: Summarize lengthy documents and translate text between languages with high accuracy.\n\nCoding Assistance: Aid in code generation and debugging for software development projects.\n\nLlama 3 is available in multiple configurations, including models with 8 billion and 70 billion parameters, catering to a range of computational requirements. \n\nBy leveraging Llama 3, developers and researchers can build advanced AI applications that require sophisticated language understanding and generation capabilities.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "71d493ca-5696-4518-865a-fe185603361d", "created_at": "2025-02-11T22:46:20.963811+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/meta-llama/llama3", "origin_model_repository_url": "https://github.com/meta-llama/llama3", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "meta-llama-3-70b", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/llama3.png", "version": "00.00.00", "model_desc": "Meta's Llama 3 is the latest iteration in Meta AI's series of large language models (LLMs), designed to advance natural language understanding and generation. Building upon its predecessors, Llama 3 offers enhanced performance and efficiency, making it a powerful tool for various applications. \n\nKey Features:\n\nInstruction Following: Llama 3 excels at understanding and executing complex instructions, making it suitable for tasks that require step-by-step guidance. \n\nKnowledge and Reasoning: The model demonstrates strong capabilities in connecting diverse ideas and providing insightful answers, akin to conversing with a knowledgeable individual. \n\nPotential Applications:\n\nText Generation: Create high-quality content across various domains, including creative writing and professional communications.\n\nConversational AI: Develop chatbots capable of engaging in natural and informative dialogues.\n\nSummarization and Translation: Summarize lengthy documents and translate text between languages with high accuracy.\n\nCoding Assistance: Aid in code generation and debugging for software development projects.\n\nLlama 3 is available in multiple configurations, including models with 8 billion and 70 billion parameters, catering to a range of computational requirements. \n\nBy leveraging Llama 3, developers and researchers can build advanced AI applications that require sophisticated language understanding and generation capabilities.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "ffeb8015-4274-4361-aafa-50abaa3cc08a", "created_at": "2025-06-09T06:49:47.897142+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/Wan-Video/Wan2.1", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "wavespeedai/wan-2-1-i2v-720p", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/cd768779-a7bf-41cd-ac59-6ab1531c1697/replicate-prediction-ce1zy3hv.webp", "version": "aa535ad6050bb18feee0e0ba99f345b0807b28baa81c95adfc4777f61f3ac41f", "model_desc": "Accelerated inference for Wan 2.1 14B image to video with high resolution, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "4973bca1-554a-4862-9c16-661db8bae302", "created_at": "2025-06-09T06:49:49.70767+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/Wan-Video/Wan2.1", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "wavespeedai/wan-2-1-i2v-480p", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/75f0346d-ec4c-4078-bb40-6705578c0d21/replicate-prediction-br080xq9.webp", "version": "ae5bc519ee414f255f66c7ac22062e01bbbd6050c04f888d002d5ee0dc087a0c", "model_desc": "Accelerated inference for Wan 2.1 14B image to video, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "9e00d0f9-6639-4ef2-ba9c-f3383fecc03a", "created_at": "2025-06-09T06:49:50.714071+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/Wan-Video/Wan2.1", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "wavespeedai/wan-2-1-t2v-480p", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/38089c83-8e9f-44e9-aa22-ed6918c85f09/replicate-prediction-vagx41q1.webp", "version": "067897beabcf7ff30fe6b10dc9f99eeebd2b4d6fdab52eb08d3b47d7116dfa19", "model_desc": "Accelerated inference for Wan 2.1 14B text to video, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "f6195d83-be2e-48cf-97f1-9da57cec1c62", "created_at": "2025-06-09T06:49:52.121347+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/Wan-Video/Wan2.1", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "wavespeedai/wan-2-1-t2v-720p", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/7103ba38-865d-45c4-9860-bf92176556c3/replicate-prediction-f9vj14ba.webp", "version": "df22ae8e2769aff950b2dfb121879bfe8068cbd96b23ee7a29d88303bf48979a", "model_desc": "Accelerated inference for Wan 2.1 14B text to video with high resolution, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "ff1bd5ac-cb97-49ca-bca0-e5aea9344faf", "created_at": "2025-06-09T06:49:53.068557+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/Wan-Video/Wan2.1", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "wan-video/wan-2-1-1-3b", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/a07f6849-fd97-4f6b-9166-887e84b0cb47/replicate-prediction-0s06z711.webp", "version": "121bbb762bf449889f090d36e3598c72c50c7a8cc2ce250433bc521a562aae61", "model_desc": "Generate 5s 480p videos. Wan is an advanced and powerful visual generation model developed by Tongyi Lab of Alibaba Group", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "59c2cf29-e0d9-470b-bc54-a933833b7034", "created_at": "2025-06-09T06:49:54.108515+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/ali-vilab/VACE", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "prunaai/vace-14b", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/e0d87648-b5af-44e0-83cc-0be03e27ccc2/wan-vace-cover.webp", "version": "bbafc615de3e3903470a335f94294810ced166309adcba307ac8692113a7b273", "model_desc": "This is VACE-14B model optimised with pruna ai. Wan2.1 VACE is an all-in-one model for video creation and editing.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "4f556e5a-f516-447f-b139-5cee2ff0bb86", "created_at": "2025-02-12T02:57:42.426509+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/zsxkib/segment-anything-2", "origin_model_repository_url": "https://github.com/facebookresearch/sam2", "model_category": ["ba09ebeb-2cda-48ee-9ece-d900ce807fb9"], "display_name": "Sam2", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/Sam2_4f556e5a-f516-447f-b139-5cee2ff0bb86/display_img.webp", "version": "00.00.00", "model_desc": "Meta's Segment Anything Model 2 (SAM 2) is an advanced AI model designed for comprehensive object segmentation in both images and videos. Building upon its predecessor, SAM 2 introduces a unified architecture that supports real-time processing and enhanced accuracy. \n\nKey Features:\nUnified Model Architecture: SAM 2 combines image and video segmentation capabilities into a single model, simplifying deployment and ensuring consistent performance across various media types. \n\nPrompt-Based Interface: The model offers a flexible prompt-based interface, allowing users to specify objects of interest through points, bounding boxes, or masks, facilitating precise and interactive segmentation. \nARXIV.ORG\n\nReal-Time Performance: Achieving inference speeds of approximately 44 frames per second, SAM 2 is suitable for applications requiring immediate feedback, such as video editing and augmented reality. \n\nZero-Shot Generalization: SAM 2 can segment objects it has never encountered before, demonstrating strong zero-shot generalization. \nARXIV.ORG\n\nPotential Applications:\nVideo Editing: Facilitates real-time object tracking and segmentation, streamlining the editing process.\nAutonomous Systems: Enhances object recognition and segmentation capabilities in real-time environments.\nMedical Imaging: Assists in accurately segmenting anatomical structures in both static images and dynamic sequences.\n\nSAM 2 is available as an open-source project, with code, model checkpoints, and example notebooks accessible through Meta's official GitHub repository. \n\nBy leveraging SAM 2, developers and researchers can implement advanced segmentation capabilities across a wide range of applications, benefiting from its unified architecture and real-time performance.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "de08314a-c4b4-4cff-b547-d9c4c8619500", "created_at": "2025-02-12T02:42:15.070161+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/TencentARC/PhotoMaker", "origin_model_repository_url": "https://github.com/TencentARC/PhotoMaker", "model_category": ["2a455e76-d55f-44fb-895c-9b49d7f18e92", "bc4bfd61-d5b5-4c93-91d4-d5c73cedc0f6"], "display_name": "tencentarc-photomaker", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/tencentarc-photomaker_de08314a-c4b4-4cff-b547-d9c4c8619500/display_img.png", "version": "00.00.00", "model_desc": "TencentARC's PhotoMaker-Style is an AI model designed to create personalized images by applying various artistic styles to user-provided photos, particularly focusing on human faces. Building upon the Stable Diffusion XL framework, it incorporates a stacked ID embedding module to ensure high-fidelity face personalization. \n\n\nKey Features:\nRapid Customization: Generates customized photos, paintings, and avatars within seconds without requiring additional training. \nArtistic Stylization: Applies a wide range of artistic styles to input images, allowing for creative transformations. \nHigh-Fidelity Personalization: Maintains the unique features of the subject's face, ensuring personalized and recognizable outputs. \n\nPotential Applications:\nAvatar Creation: Design personalized avatars in various artistic styles for social media profiles or gaming.\nArtistic Rendering: Transform standard photos into stylized artworks for creative projects or personal use.\nContent Generation: Assist artists and designers in generating inspiration or base images for further development.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "532ea401-4137-40bf-96b9-d4adbcd5a96c", "created_at": "2025-02-12T10:11:31.981459+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/salesforce/BLIP", "origin_model_repository_url": "https://github.com/salesforce/BLIP", "model_category": ["585b85e6-83bc-456a-9b7a-91beb2644da4", "e0c5e9d0-6deb-4828-bbb0-89ebaacd9dad"], "display_name": "Blip", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/blip2.png", "version": "00.00.00", "model_desc": "Salesforce's Bootstrapping Language-Image Pre-training (BLIP) is a vision-language pre-training framework designed to unify understanding and generation tasks across both modalities. BLIP effectively utilizes noisy web data by generating synthetic captions and filtering out the noisy ones, enabling it to learn from large-scale, uncurated datasets. \n\n\nPotential Applications:\n\nImage-Text Retrieval: Enhances the ability to match images with corresponding textual descriptions and vice versa.\nImage Captioning: Generates accurate and contextually relevant descriptions for images.\nVisual Question Answering (VQA): Enables systems to answer questions based on visual content.\nVisual Reasoning and Dialogue: Facilitates complex reasoning tasks and interactive dialogues involving visual information.\n\nBy leveraging BLIP, developers and researchers can build advanced AI applications that require a deep understanding of both visual and textual data, benefiting from its unified approach to vision-language pre-training.", "used_models": [], "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "6a8d53ec-ff7b-46bb-83c7-959a05cc2212", "created_at": "2025-06-09T06:49:55.423615+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/ali-vilab/VACE", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "prunaai/vace-1-3b", "display_img_url": "https://replicate.delivery/xezq/qRl0z5qTqYLoIhWIR2foz0kaFKpSmLAIxrRl971ZgSffZXapA/output.mp4", "version": "ecf7f253f49c527c2baaf22d630685b7b877e302a43cae4c97a623c0ef906bd7", "model_desc": "This is VACE-1.3B model optimised with pruna ai. Wan2.1 VACE is an all-in-one model for video creation and editing.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "acc20303-ba33-4b79-bc65-23f9134d535e", "created_at": "2025-06-09T06:49:57.053492+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/andreasjansson/Wan2.1", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "andreasjansson/wan-1-3b-inpaint", "display_img_url": "https://tjzk.replicate.delivery/models_models_cover_image/d635b2b1-839f-4af7-a206-7b2af8cc5878/video.webp", "version": "7abfdb3370aba087f9a5eb8b733c2174bc873a957e5c2c4835767247287dbf89", "model_desc": "Inpainting and video2video experiments with Wan 2.1", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "db28e63a-d62d-4110-8d0d-fb2de34e1bff", "created_at": "2025-06-09T06:49:58.015897+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/wanvideo", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "lucataco/wan-2-1-1-3b-vid2vid", "display_img_url": "https://tjzk.replicate.delivery/models_models_cover_image/dc302e8f-162c-4ef0-96f9-b7d4a840c904/sunglasses.webp", "version": "9349766527ed95fa6194dcca4cae3d497357e207025beb0b97fb0403420142b8", "model_desc": "Wan 2.1 1.3b Video to Video. Wan is a powerful visual generation model developed by Tongyi Lab of Alibaba Group", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "3d757d74-8d69-46c5-9dc0-988b635ebed8", "created_at": "2025-06-09T06:49:59.391401+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/replicate/cog-comfyui-wan-with-lora", "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "fofr/wan2-1-with-lora", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/6069c99f-38c7-433a-8a38-d6bd906e6773/wan-lora-cover.webp", "version": "c48fa8ec65b13143cb552ab98ea17984eab9d70e9fe99479117de40a2a7f9ed0", "model_desc": "Run Wan2.1 14b or 1.3b with a lora", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": true}, {"id": "d3511bff-ce19-4d2a-b699-06f9cfef36d1", "created_at": "2025-06-09T06:50:00.627933+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "anthropic/claude-3-7-sonnet", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/56aed331-fb30-4e82-9708-b63b2fa90699/claude-3.7-logo.webp", "version": "81a891bd00c339f3565bda15b255b372eb8bf6c669fe996b66eea5d677454a46", "model_desc": "The most intelligent Claude model and the first hybrid reasoning model on the market (claude-3-7-sonnet-20250219)", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "f419329c-c9af-4f62-9b31-72ef6f60ac5b", "created_at": "2025-02-11T23:49:10.993798+00:00", "created_by": "b8c2ae59-50ad-423d-8e09-28b44119bef1", "github_url": "https://github.com/deepseek-ai/DeepSeek-R1", "origin_model_repository_url": "https://github.com/deepseek-ai/DeepSeek-R1", "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "deepseek-r1", "display_img_url": "https://socaityfiles.blob.core.windows.net/backend-model-meta/deepseek-r1_f419329c-c9af-4f62-9b31-72ef6f60ac5b/display_img.png", "version": "00.00.00", "model_desc": "DeepSeek-R1 is an open-source reasoning model developed by the Chinese AI company DeepSeek, designed to tackle complex tasks requiring logical inference, mathematical problem-solving, and real-time decision-making. \nDEEPSEEKR1.ORG\n\nKey Features:\n\nAdvanced Reasoning: Utilizes reinforcement learning to develop sophisticated reasoning behaviors, including self-verification and reflection. \n\nMathematical Proficiency: Achieves high accuracy in mathematical tasks, with a 79.8% precision on the American Invitational Mathematics Examination (AIME) 2024 and 97.3% on the MATH-500 benchmark. \n\nProgramming Expertise: Demonstrates expert-level coding capabilities, surpassing 96.3% of human participants on Codeforces with an Elo rating of 2029. \n\n\nPotential Applications:\n\nProblem-Solving: Assists in solving complex mathematical and logical problems.\n\nCode Generation and Analysis: Aids in writing, optimizing, and analyzing code across multiple programming languages.\n\nDecision Support Systems: Enhances real-time decision-making processes in various domains.\n\nDeepSeek-R1 is available under the MIT license, allowing for free use, modification, and distribution. The model and its distilled versions can be accessed through DeepSeek's official channels. \n\n\nBy leveraging DeepSeek-R1, developers and researchers can build advanced AI applications that require sophisticated reasoning and problem-solving capabilities.", "used_models": [], "sort_idx_featured": 0, "n_usages": 0, "is_public": true, "is_validated": false, "has_billing": true}, {"id": "3142506a-00f7-4160-a375-fea779797093", "created_at": "2025-06-09T06:50:01.892272+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["2a455e76-d55f-44fb-895c-9b49d7f18e92"], "display_name": "ideogram-ai/ideogram-v2a-turbo", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/b97c9ffa-3332-4476-9fb2-062bf23038ac/replicate-prediction-hw96eg13.webp", "version": "1978f14e3f7f5406243c31233a0456216262e0127e6cca571b6d9fc57f52d279", "model_desc": "Like Ideogram v2 turbo, but now faster and cheaper", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "c4e71a83-16e2-4697-8302-6298d2c13c9e", "created_at": "2025-06-09T06:50:02.813386+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["2a455e76-d55f-44fb-895c-9b49d7f18e92"], "display_name": "ideogram-ai/ideogram-v2a", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/6e45e974-f381-435a-b9dd-23f3e6801c19/replicate-prediction-1yv65m0q.webp", "version": "4f8774263c3eef920c94916a3cb6a064ed04b06a7aef2af1ffebe0803ada3497", "model_desc": "Like Ideogram v2, but faster and cheaper", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "2fccf4d8-e610-4d3c-bff6-e99af4cbfcf4", "created_at": "2025-06-09T06:50:03.684853+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "google/veo-2", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/d58d1243-1045-4cb0-ab94-454757139c9d/replicate-prediction-pgghdpmn.webp", "version": "0546b4e90d4f215c7fc42c9b8d815df3a7b6e47a754ee94ddb62b3f667fea9a5", "model_desc": "State of the art video generation model. Veo 2 can faithfully follow simple and complex instructions, and convincingly simulates real-world physics as well as a wide range of visual styles.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "cee064b9-f65d-41a8-8cc6-ae873bf143e4", "created_at": "2025-06-09T06:50:04.889202+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["2a455e76-d55f-44fb-895c-9b49d7f18e92"], "display_name": "minimax/image-01", "display_img_url": "https://tjzk.replicate.delivery/models_models_cover_image/926994db-2c8e-4b7d-934f-2f86b2480e55/43b05178-4b2a-42d9-9130-4fedae65.webp", "version": "abbfceebd9f32eba5a69624c59fe3d434ed8d8208aa858f9734040062c03d8c9", "model_desc": "Minimax's first image model, with character reference support", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "6405a700-074b-4f83-a8e8-93e65971239e", "created_at": "2025-06-09T06:50:06.277391+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "luma/ray-flash-2-720p", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/05ed2738-a4ed-412e-a4c9-fd2ce48f4e3d/replicate-prediction-af9qsmzk.webp", "version": "0f05265c51f0221b8941b72fd8bfbc2200947931accc80f071665c3949e5531c", "model_desc": "Generate 5s and 9s 720p videos, faster and cheaper than Ray 2", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "63c2fb14-d3e5-457c-9494-8cf2889e9642", "created_at": "2025-06-09T06:50:07.406435+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["7dde04c1-37ed-4779-969f-82b4b7988d9c"], "display_name": "luma/ray-flash-2-540p", "display_img_url": "https://replicate.delivery/xezq/vfuqZQoMXX3MfkaUsfoqanFPHaB42R7Mbq7i0bc7H5zX52woA/tmpd2p3nej6.mp4", "version": "984c65a22737bce2728b6fc1ab4e40d7219bb9faf5f71417db63f4a3ea7c4605", "model_desc": "Generate 5s and 9s 540p videos, faster and cheaper than Ray 2", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "088e54e6-1f6d-4ad1-80cb-d1e0934a96ab", "created_at": "2025-06-09T06:50:10.347537+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": "https://github.com/bjhargrave/cog-models/tree/main/ibm-granite/granite-3.3-8b-instruct", "origin_model_repository_url": null, "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "ibm-granite/granite-3-3-8b-instruct", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/81dc0178-9ff8-4869-9c72-cd334450533e/Granite_2D_B_Low_no_Grain.png", "version": "a325a0cacfb0aa9226e6bad1abe5385f1073f4c7f8c36e52ed040e5409e6c034", "model_desc": "Granite-3.3-8B-Instruct is a 8-billion parameter 128K context length language model fine-tuned for improved reasoning and instruction-following capabilities.", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "9b35c477-bfba-4ffe-8707-af1bcb6cb42d", "created_at": "2025-06-09T06:50:11.816528+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["01dd20d6-db6a-4c59-9cee-1a3860356a88"], "display_name": "meta/llama-4-maverick-instruct", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/98360ab9-dce0-4404-89ea-33aac8976393/meta-logo.png", "version": "25bdfe11f52b557ade65599fad30b1a6f6d87ede91043974bbd320d6a4c1c841", "model_desc": "A 17 billion parameter model with 128 experts", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}, {"id": "74d8a824-ceeb-4aeb-b39e-df039dcb17a3", "created_at": "2025-06-09T06:50:16.323959+00:00", "created_by": "0a93956c-2365-49a1-8411-fafcd3bca66a", "github_url": null, "origin_model_repository_url": null, "model_category": ["0e50e16b-a3e3-443f-8bcd-d48b25d57ff6"], "display_name": "topazlabs/image-upscale", "display_img_url": "https://tjzk.replicate.delivery/models_models_featured_image/e220ac1a-3d43-4c48-884f-5678c0c3ea94/tmp9lnlmaoj.jpg", "version": "2fdc3b86a01d338ae89ad58e5d9241398a8a01de9b0dda41ba8a0434c8a00dc3", "model_desc": "Professional-grade image upscaling, from Topaz Labs", "used_models": null, "sort_idx_featured": null, "n_usages": 0, "is_public": true, "is_validated": true, "has_billing": false}]